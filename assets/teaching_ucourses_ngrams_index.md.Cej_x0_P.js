import{_ as e}from"./chunks/pdf.ootvOnpX.js";import{_ as r}from"./chunks/slides.Cre3B-PD.js";import{_ as i,c as s,a1 as o,o as t}from"./chunks/framework.xETXYpcG.js";const b=JSON.parse('{"title":"Word Prediction with N-Grams Model Using Python","description":"","frontmatter":{"prev":false,"next":false,"image":"tokyo.jpg","caption":"Tokyo, Japan, April 2017"},"headers":[],"relativePath":"teaching/ucourses/ngrams/index.md","filePath":"teaching/ucourses/ngrams/index.md"}'),l={name:"teaching/ucourses/ngrams/index.md"};function n(c,a,d,m,h,u){return t(),s("div",null,a[0]||(a[0]=[o('<h1 id="word-prediction-with-n-grams-model-using-python" tabindex="-1">Word Prediction with N-Grams Model Using Python <a class="header-anchor" href="#word-prediction-with-n-grams-model-using-python" aria-label="Permalink to &quot;Word Prediction with N-Grams Model Using Python&quot;">​</a></h1><p>The “AI611µ Word Prediction with N-Grams Model using Python” micro-course is about how to perform <strong>word prediction</strong> with artificial intelligence techniques. More precisely, it presents how to use <strong>N-Grams model</strong> to predict words based on a corpus and builds a small example with Python.</p><p>I gave this micro-course in 2020, once, at the <a href="https://www.ecam.be" target="_blank" rel="noreferrer">ECAM Brussels Engineering School</a> (ECAM) as a part of the <a href="./../../ecam/ai/">“I4110 – Artificial intelligence” course</a>. The course is taught in French, but all the material is available in English and in French.</p><h2 id="documents" tabindex="-1">Documents <a class="header-anchor" href="#documents" aria-label="Permalink to &quot;Documents&quot;">​</a></h2><ul><li>General information about the micro-course</li><li><a href="/files/ecam/general/ECAM-Competency-Based-Assessment-Slides.pdf" target="_blank">Competency Based Assessment <img src="'+e+'" alt="PDF"></a></li><li><a href="/files/ucourses/ngrams/NGramsModel-Competencies-List.pdf" target="_blank">Grid of skills to acquire <img src="'+e+'" alt="PDF"></a></li></ul><h2 id="theory" tabindex="-1">Theory <a class="header-anchor" href="#theory" aria-label="Permalink to &quot;Theory&quot;">​</a></h2><ul><li><a href="/files/ucourses/ngrams/NGramsModel-Session1-Slides.pdf" target="_blank">Session 1: Word Prediction Problem and N-Grams Model <img src="'+r+'" alt="Slides"></a></li><li><a href="/files/ucourses/ngrams/NGramsModel-Session2-Slides.pdf" target="_blank">Session 2: N-Grams Model Training and Model Evaluation <img src="'+r+'" alt="Slides"></a></li></ul><h2 id="practice" tabindex="-1">Practice <a class="header-anchor" href="#practice" aria-label="Permalink to &quot;Practice&quot;">​</a></h2><ul><li><a href="/files/ucourses/ngrams/NGramsModel-Quizz1.pdf" target="_blank">Quizz 1: N-Grams model <img src="'+e+'" alt="PDF"></a></li><li>Quizz 2: Bigram model training</li><li>Quizz 3: N-Grams model training and evaluation</li><li><a href="/files/ucourses/ngrams/NGramsModel-Coding1.pdf" target="_blank">Coding 1: Corpus statistics <img src="'+e+'" alt="PDF"></a></li><li><a href="/files/ucourses/ngrams/NGramsModel-Coding2.pdf" target="_blank">Coding 2: Training a bigram model <img src="'+e+'" alt="PDF"></a></li><li><a href="/files/ucourses/ngrams/NGramsModel-Mission1.pdf" target="_blank">Mission 1: N-Grams model applications <img src="'+e+'" alt="PDF"></a></li><li><a href="/files/ucourses/ngrams/NGramsModel-Mission2.pdf" target="_blank">Mission 2: Bigram model training with nltk <img src="'+e+'" alt="PDF"></a></li><li><a href="/files/ucourses/ngrams/NGramsModel-Project1.pdf" target="_blank">Project 1: Simple word prediction application <img src="'+e+'" alt="PDF"></a></li></ul><h2 id="resources" tabindex="-1">Resources <a class="header-anchor" href="#resources" aria-label="Permalink to &quot;Resources&quot;">​</a></h2><p>This section gathers <strong>resources</strong> that have been used to create this micro-course. These latter can be used to learn more about N-Grams.</p><h3 id="reference-books" tabindex="-1">Reference books <a class="header-anchor" href="#reference-books" aria-label="Permalink to &quot;Reference books&quot;">​</a></h3><ul><li>Daniel Jurafsky and James H. Martin (2008). Speech and Language Processing (Second Edition). Pearson. <small>(ISBN: 978-0-135-04196-3)</small></li></ul><h3 id="online-resources" tabindex="-1">Online resources <a class="header-anchor" href="#online-resources" aria-label="Permalink to &quot;Online resources&quot;">​</a></h3><ul><li><a href="http://guidetodatamining.com/ngramAnalyzer" target="_blank" rel="noreferrer">Online NGram Analyzer</a> to perform simple statistics on texts.</li><li><a href="https://books.google.com/ngrams" target="_blank" rel="noreferrer">Google Ngram Viewer</a> with data collected from Google Books.</li><li>Official website of the <a href="https://www.nltk.org" target="_blank" rel="noreferrer">Natural Language Toolkit (NLTK) Python module</a>.</li></ul>',15)]))}const k=i(l,[["render",n]]);export{b as __pageData,k as default};
